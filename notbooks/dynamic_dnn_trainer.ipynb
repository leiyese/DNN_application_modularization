{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PROJECT_ROOT_ON_DRIVE = '/content/drive/MyDrive/your_colab_projects/dynamic_dnn_trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option A: If cloning from GitHub/GitLab ---\n",
    "# !git clone <your_repository_url> dynamic_dnn_trainer_colab\n",
    "# %cd dynamic_dnn_trainer_colab\n",
    "# !pip install -q -r requirements.txt # Ensure requirements.txt is in your repo\n",
    "\n",
    "# --- Option B: If you uploaded a zip of your project ---\n",
    "# !unzip dynamic_dnn_trainer.zip -d project_files\n",
    "# %cd project_files/dynamic_dnn_trainer\n",
    "# !pip install -q -r requirements.txt\n",
    "\n",
    "# --- Option C: If project is already in Colab environment (e.g., from Drive mount) ---\n",
    "# Just ensure you are in the project's root directory.\n",
    "# For example, if PROJECT_ROOT_ON_DRIVE was set:\n",
    "# %cd $PROJECT_ROOT_ON_DRIVE\n",
    "# !pip install -q -r requirements.txt # Still good to ensure packages\n",
    "\n",
    "# Verify current directory\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add src to Python path\n",
    "# Assumes the notebook is run from the project root directory where 'src' is a subfolder\n",
    "src_path = str(Path(os.getcwd()) / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "print(f\"Added '{src_path}' to sys.path\")\n",
    "\n",
    "# Import from our custom modules\n",
    "try:\n",
    "    from src import config\n",
    "    from src.data_ingestion import loader\n",
    "    from src.eda import exploratory_analysis as eda\n",
    "    from src.preprocessing import transformers\n",
    "    from src.modeling import dnn_builder, trainer\n",
    "    from src.evaluation import plots\n",
    "    from src.utils import helpers\n",
    "    print(\"Successfully imported all custom modules.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"Ensure you are in the project root and 'src' is in sys.path.\")\n",
    "    print(f\"Current sys.path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- TensorFlow Version: {tf.__version__} ---\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"--- GPU(s) available: {len(gpu_devices)} ---\")\n",
    "    for i, device in enumerate(gpu_devices):\n",
    "        print(f\"  GPU {i}: Name={device.name}, Type={device.device_type}\")\n",
    "    # Colab usually assigns GPU:0 if a GPU runtime is selected\n",
    "    try:\n",
    "        with tf.device('/GPU:0'): # Standard name for Colab GPU\n",
    "            tf.constant([[1.0]]) # Simple op\n",
    "        print(\"--- GPU test op successful. GPU is likely active. ---\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"--- GPU test op failed: {e}. May fall back to CPU. ---\")\n",
    "else:\n",
    "    print(\"--- No GPU available (or not a GPU runtime). TensorFlow will use CPU. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_EDA_PLOTS_DIR = config.PLOTS_OUTPUT_DIR / \"notebook_eda_output\"\n",
    "NOTEBOOK_MODEL_SAVE_PATH = config.MODEL_OUTPUT_DIR / \"notebook_dnn_model_multiclass.keras\"\n",
    "NOTEBOOK_HISTORY_SAVE_PATH = config.PROCESSED_DATA_DIR / \"notebook_dnn_training_history_multiclass.pkl\"\n",
    "NOTEBOOK_HISTORY_PLOT_PATH = config.PLOTS_OUTPUT_DIR / \"notebook_dnn_training_history_multiclass.png\"\n",
    "NOTEBOOK_TENSORBOARD_LOG_DIR = config.PROJECT_ROOT / \"outputs\" / \"logs\" / \"notebook_dnn_multiclass_run\"\n",
    "NOTEBOOK_PREPROCESSOR_SAVE_PATH = config.PROCESSED_DATA_DIR / \"notebook_feature_preprocessor.pkl\"\n",
    "\n",
    "\n",
    "# Ensure directories exist using our helper\n",
    "helpers.ensure_directory_exists(NOTEBOOK_EDA_PLOTS_DIR)\n",
    "helpers.ensure_directory_exists(NOTEBOOK_MODEL_SAVE_PATH.parent)\n",
    "helpers.ensure_directory_exists(NOTEBOOK_HISTORY_SAVE_PATH.parent)\n",
    "helpers.ensure_directory_exists(NOTEBOOK_HISTORY_PLOT_PATH.parent)\n",
    "helpers.ensure_directory_exists(NOTEBOOK_TENSORBOARD_LOG_DIR)\n",
    "helpers.ensure_directory_exists(NOTEBOOK_PREPROCESSOR_SAVE_PATH.parent)\n",
    "\n",
    "print(\"Notebook output directories ensured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> PART 1: Data Ingestion and Preprocessing <<<\")\n",
    "\n",
    "# 1.1 Load Raw Data\n",
    "print(\"\\n--- 1.1 Loading Raw Data ---\")\n",
    "raw_df = loader.load_data_from_csv(csv_file_path=config.RAW_DATA_FILE_PATH)\n",
    "if raw_df is None:\n",
    "    raise ValueError(\"Failed to load raw data. Halting notebook execution.\")\n",
    "print(f\"Raw data loaded. Shape: {raw_df.shape}\")\n",
    "display(raw_df.head()) # Use display for better rendering in notebooks\n",
    "\n",
    "# 1.2 Perform Exploratory Data Analysis (EDA)\n",
    "# Set run_eda = True or False as desired for the notebook run\n",
    "run_eda_notebook = True\n",
    "if run_eda_notebook:\n",
    "    print(\"\\n--- 1.2 Performing Exploratory Data Analysis (EDA) ---\")\n",
    "    _ = eda.generate_descriptive_statistics(raw_df.copy(), df_name=\"Raw Data (Notebook)\")\n",
    "    _ = eda.count_missing_values(raw_df.copy(), df_name=\"Raw Data (Notebook)\")\n",
    "    \n",
    "    existing_numerical_for_plot = [col for col in config.NUMERICAL_FEATURES if col in raw_df.columns]\n",
    "    if existing_numerical_for_plot:\n",
    "        eda.plot_histograms(\n",
    "            df=raw_df.copy(),\n",
    "            columns_to_plot=existing_numerical_for_plot,\n",
    "            plot_save_directory=NOTEBOOK_EDA_PLOTS_DIR # Save to notebook specific dir\n",
    "        )\n",
    "        eda.plot_correlation_heatmap(\n",
    "            df=raw_df.copy(),\n",
    "            columns_for_corr=existing_numerical_for_plot,\n",
    "            plot_save_path=NOTEBOOK_EDA_PLOTS_DIR / \"correlation_matrix_notebook.png\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No numerical features from config found for EDA plots.\")\n",
    "    print(f\"EDA plots (if any) saved to: {NOTEBOOK_EDA_PLOTS_DIR}\")\n",
    "else:\n",
    "    print(\"\\n--- Skipping EDA for this notebook run ---\")\n",
    "\n",
    "# 1.3 Split Data\n",
    "print(\"\\n--- 1.3 Splitting Data ---\")\n",
    "X_train_raw_df, X_test_raw_df, y_train_raw_series, y_test_raw_series = \\\n",
    "    transformers.split_dataframe_into_train_test(\n",
    "        dataframe=raw_df,\n",
    "        target_column_name=config.TARGET_COLUMN,\n",
    "        test_set_ratio=config.TEST_SET_SIZE,\n",
    "        random_seed=config.RANDOM_SEED,\n",
    "        stratify_by_target=True\n",
    "    )\n",
    "\n",
    "# 1.4 Create Feature Preprocessor\n",
    "print(\"\\n--- 1.4 Creating Feature Preprocessor ---\")\n",
    "feature_preprocessor_obj = transformers.create_feature_preprocessor(\n",
    "    numerical_cols=config.NUMERICAL_FEATURES,\n",
    "    categorical_cols=config.CATEGORICAL_FEATURES\n",
    ")\n",
    "\n",
    "# 1.5 Apply Feature Preprocessing (Output to NumPy)\n",
    "print(\"\\n--- 1.5 Applying Feature Preprocessing ---\")\n",
    "X_train_np, X_test_np = transformers.apply_feature_preprocessing_to_numpy(\n",
    "    X_train_df=X_train_raw_df.copy(),\n",
    "    X_test_df=X_test_raw_df.copy(),\n",
    "    preprocessor_object=feature_preprocessor_obj,\n",
    "    fit_preprocessor_on_train=True\n",
    ")\n",
    "if helpers.save_object_as_pickle(feature_preprocessor_obj, NOTEBOOK_PREPROCESSOR_SAVE_PATH):\n",
    "    print(f\"Fitted feature preprocessor saved to: {NOTEBOOK_PREPROCESSOR_SAVE_PATH}\")\n",
    "\n",
    "# 1.6 Prepare Target Variable (Output to NumPy)\n",
    "print(\"\\n--- 1.6 Preparing Target Variable ---\")\n",
    "y_train_np = transformers.prepare_target_to_numpy(y_train_raw_series.copy())\n",
    "y_test_np = transformers.prepare_target_to_numpy(y_test_raw_series.copy())\n",
    "\n",
    "print(f\"Data prepared. X_train_np shape: {X_train_np.shape}, y_train_np shape: {y_train_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a256bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n>>> PART 2: DNN Modeling, Training & Basic Evaluation <<<\")\n",
    "\n",
    "# 2.1 Build DNN Model\n",
    "print(\"\\n--- 2.1 Building DNN Model ---\")\n",
    "input_shape_for_model = (X_train_np.shape[1],)\n",
    "dnn_model = dnn_builder.build_dynamic_dnn_model(\n",
    "    input_features_shape=input_shape_for_model,\n",
    "    model_arch_params=config.DNN_MODEL_PARAMS[\"architecture\"],\n",
    "    compilation_params=config.DNN_MODEL_PARAMS[\"compilation\"]\n",
    ")\n",
    "\n",
    "# 2.2 Train DNN Model\n",
    "print(\"\\n--- 2.2 Training DNN Model ---\")\n",
    "# Use test set as validation data for callbacks in this notebook example\n",
    "validation_data_for_training_nb = (X_test_np, y_test_np)\n",
    "\n",
    "notebook_callbacks = trainer.create_standard_callbacks(\n",
    "    early_stopping_params={\"monitor\": \"val_accuracy\", \"patience\": 10, \"restore_best_weights\": True},\n",
    "    model_checkpoint_filepath=NOTEBOOK_MODEL_SAVE_PATH,\n",
    "    model_checkpoint_params={\"monitor\": \"val_accuracy\", \"save_best_only\": True},\n",
    "    tensorboard_logdir=NOTEBOOK_TENSORBOARD_LOG_DIR\n",
    ")\n",
    "\n",
    "# Use training params from config, can override epochs for notebook if desired\n",
    "notebook_training_params = config.DNN_MODEL_PARAMS[\"training\"].copy()\n",
    "# notebook_training_params[\"epochs\"] = 5 # Example: Shorter run for notebook\n",
    "\n",
    "trained_model_nb, training_history_nb = trainer.train_keras_model(\n",
    "    model_to_train=dnn_model,\n",
    "    X_train_data=X_train_np,\n",
    "    y_train_data=y_train_np,\n",
    "    training_params=notebook_training_params,\n",
    "    validation_data_tuple=validation_data_for_training_nb,\n",
    "    callbacks_to_use=notebook_callbacks,\n",
    "    history_log_path=NOTEBOOK_HISTORY_SAVE_PATH\n",
    ")\n",
    "if NOTEBOOK_MODEL_SAVE_PATH.exists():\n",
    "    print(f\"Model training complete. Best model saved to: {NOTEBOOK_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# 2.3 Plot Training History\n",
    "print(\"\\n--- 2.3 Plotting Training History ---\")\n",
    "if training_history_nb and training_history_nb.history:\n",
    "    plots.plot_training_history(\n",
    "        history_data=training_history_nb.history,\n",
    "        plot_title=\"DNN Model Training History (Notebook)\",\n",
    "        save_plot_path=NOTEBOOK_HISTORY_PLOT_PATH # Saves the plot\n",
    "    )\n",
    "    # To display inline in Colab after saving (optional):\n",
    "    # from IPython.display import Image\n",
    "    # display(Image(filename=NOTEBOOK_HISTORY_PLOT_PATH))\n",
    "else:\n",
    "    print(\"Skipping plotting: no training history data.\")\n",
    "\n",
    "print(\"\\n--- Notebook Pipeline for Parts 1 & 2 COMPLETED ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231ec11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
